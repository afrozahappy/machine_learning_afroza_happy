{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'telecom_churn_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a7dd3ffb5dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtelecom_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"telecom_churn_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#reading dataframe from csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'telecom_churn_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "telecom_df=pd.read_csv(\"telecom_churn_data.csv\",  encoding = \"ISO-8859-1\")#reading dataframe from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with 0 values and columns not required for modelling\n",
    "telecom_df=telecom_df.drop(columns=['mobile_number','circle_id','loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting categorical and continuous variables\n",
    "cont_tel_df=telecom_df.select_dtypes(include=['int64','float64'])\n",
    "cat_tel_df=telecom_df.select_dtypes(include=['object'])\n",
    "print(cont_tel_df.columns)\n",
    "print(cat_tel_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all date columns\n",
    "telecom_df=telecom_df.drop(columns=cat_tel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting columns with 50% null values\n",
    "cols=telecom_df.loc[:,telecom_df.isna().mean()>=.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with 50% null values\n",
    "telecom_df=telecom_df.drop(columns=['max_rech_data_6', 'max_rech_data_7',\n",
    "       'max_rech_data_8', 'max_rech_data_9', 'count_rech_2g_6',\n",
    "       'count_rech_2g_7', 'count_rech_2g_8', 'count_rech_2g_9',\n",
    "       'count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8',\n",
    "       'count_rech_3g_9', 'av_rech_amt_data_6', 'av_rech_amt_data_7',\n",
    "       'av_rech_amt_data_8', 'av_rech_amt_data_9', 'arpu_3g_6', 'arpu_3g_7',\n",
    "       'arpu_3g_8', 'arpu_3g_9', 'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8',\n",
    "       'arpu_2g_9', 'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8',\n",
    "       'night_pck_user_9', 'fb_user_6', 'fb_user_7', 'fb_user_8', 'fb_user_9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting columns with any null values\n",
    "cols=telecom_df.loc[:,telecom_df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with any 50% values\n",
    "telecom_df = telecom_df.dropna(thresh=53)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_1=telecom_df[cols.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null values with 0\n",
    "telecom_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting columns with any null values\n",
    "cols=telecom_df.loc[:,telecom_df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "telecom_df['total_amt_6']=telecom_df['total_rech_amt_6']+telecom_df['total_rech_data_6']# calculating total amount spent in recharge 6th month\n",
    "telecom_df['total_amt_7']=telecom_df['total_rech_amt_7']+telecom_df['total_rech_data_7']# calculating total amount spent in recharge 7th month\n",
    "telecom_df['total_amt_8']=telecom_df['total_rech_amt_8']+telecom_df['total_rech_data_8']# calculating total amount spent in recharge 8th month\n",
    "telecom_df['total_amt_9']=telecom_df['total_rech_amt_9']+telecom_df['total_rech_data_9']# calculating total amount spent in recharge 9th month\n",
    "telecom_df['total_usage']=telecom_df['total_og_mou_9']+telecom_df['total_ic_mou_9']+telecom_df['vol_2g_mb_9']+telecom_df['vol_3g_mb_9']#calculating total usage of calls and data in 9th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating average recharge amount of 6th and 7th month\n",
    "average_rec_amt=(telecom_df['total_amt_6'].sum()+telecom_df['total_amt_7'].sum())/(2*len(telecom_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rec_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering top 30% customers\n",
    "telecom_df=telecom_df.loc[(telecom_df.total_amt_6+telecom_df.total_amt_7)/2 >=(average_rec_amt*.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all columns of 9th month\n",
    "sep_cols = [col for col in telecom_df.columns if '9' in col or 'sep' in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all columns of 9th month\n",
    "telecom_df=telecom_df.drop(columns=sep_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting columns with 90% 0 values\n",
    "telecom_df.quantile(0.9)\n",
    "drop_cols=[col for col in telecom_df.columns if telecom_df[col].quantile(0.9)<=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting churn customers and the target variable\n",
    "telecom_df['churn'] = telecom_df.total_usage.apply(lambda x: 1 if x == 0.0 else 0)\n",
    "telecom_df=telecom_df.drop(columns='total_usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of churn customers\n",
    "len(telecom_df.loc[telecom_df['churn']==1])/len(telecom_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with 90% 0 values\n",
    "telecom_df=telecom_df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 1st 20 columns\n",
    "telecom_df.iloc[:,0:20].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 1st 20 columns\n",
    "telecom_df=telecom_df.loc[telecom_df.arpu_6<=telecom_df.arpu_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.arpu_7<=telecom_df.arpu_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.arpu_8<=telecom_df.arpu_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.onnet_mou_6<=telecom_df.onnet_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.onnet_mou_7<=telecom_df.onnet_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.onnet_mou_8<=telecom_df.onnet_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.offnet_mou_6<=telecom_df.offnet_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.offnet_mou_7<=telecom_df.offnet_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.offnet_mou_8<=telecom_df.offnet_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.roam_ic_mou_6<=telecom_df.roam_ic_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.roam_ic_mou_7<=telecom_df.roam_ic_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.roam_ic_mou_8<=telecom_df.roam_ic_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.roam_og_mou_6<=telecom_df.roam_og_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.roam_og_mou_7<=telecom_df.roam_og_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.roam_og_mou_8<=telecom_df.roam_og_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2t_mou_6<=telecom_df.loc_og_t2t_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2t_mou_7<=telecom_df.loc_og_t2t_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2t_mou_8<=telecom_df.loc_og_t2t_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2m_mou_6<=telecom_df.loc_og_t2m_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2m_mou_7<=telecom_df.loc_og_t2m_mou_7.quantile(0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.iloc[:,0:20].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 20th-40th columns\n",
    "telecom_df.iloc[:,20:40].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 20th-40th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2m_mou_8<=telecom_df.loc_og_t2m_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2f_mou_6<=telecom_df.loc_og_t2f_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2f_mou_7<=telecom_df.loc_og_t2f_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2f_mou_8<=telecom_df.loc_og_t2f_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2c_mou_6<=telecom_df.loc_og_t2c_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2c_mou_7<=telecom_df.loc_og_t2c_mou_7.quantile(0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.iloc[:,0:20].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.iloc[:,20:40].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 20th-40th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_t2c_mou_8<=telecom_df.loc_og_t2c_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_mou_6<=telecom_df.loc_og_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_og_mou_7<=telecom_df.loc_og_mou_7.quantile(0.99)]\n",
    "\n",
    "telecom_df=telecom_df.loc[telecom_df.std_og_mou_6<=telecom_df.std_og_mou_6.quantile(0.99)]\n",
    "\n",
    "telecom_df=telecom_df.drop(columns=['std_og_t2f_mou_6','std_og_t2f_mou_7','std_og_t2f_mou_8','og_others_6'])#dropping cokumns with maximum 0 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 20th-40th columns\n",
    "telecom_df.iloc[:,20:40].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 20th-40th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.std_og_mou_7<=telecom_df.std_og_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_og_mou_8<=telecom_df.std_og_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.spl_og_mou_6<=telecom_df.spl_og_mou_6.quantile(0.99)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "telecom_df.iloc[:,20:40].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 40th-60th columns\n",
    "telecom_df.iloc[:,40:60].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 40th-60th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.spl_og_mou_7<=telecom_df.spl_og_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.spl_og_mou_8<=telecom_df.spl_og_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_og_mou_6<=telecom_df.total_og_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_og_mou_8<=telecom_df.total_og_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2t_mou_6<=telecom_df.loc_ic_t2t_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2t_mou_7<=telecom_df.loc_ic_t2t_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2t_mou_8<=telecom_df.loc_ic_t2t_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2m_mou_6<=telecom_df.loc_ic_t2m_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2m_mou_7<=telecom_df.loc_ic_t2m_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2m_mou_8<=telecom_df.loc_ic_t2m_mou_8.quantile(0.99)]\n",
    "\n",
    "telecom_df.iloc[:,40:60].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 40th-60th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2f_mou_6<=telecom_df.loc_ic_t2f_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2f_mou_7<=telecom_df.loc_ic_t2f_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_t2f_mou_8<=telecom_df.loc_ic_t2f_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_mou_6<=telecom_df.loc_ic_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_mou_7<=telecom_df.loc_ic_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.loc_ic_mou_8<=telecom_df.loc_ic_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2t_mou_6<=telecom_df.std_ic_t2t_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2t_mou_7<=telecom_df.std_ic_t2t_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2t_mou_8<=telecom_df.std_ic_t2t_mou_8.quantile(0.99)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 40th-60th columns\n",
    "telecom_df.iloc[:,40:60].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])\n",
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 60th-80th columns\n",
    "telecom_df.iloc[:,60:80].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 60th-80th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2m_mou_6<=telecom_df.std_ic_t2m_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2m_mou_7<=telecom_df.std_ic_t2m_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2m_mou_8<=telecom_df.std_ic_t2m_mou_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2f_mou_6<=telecom_df.std_ic_t2f_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2f_mou_7<=telecom_df.std_ic_t2f_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.std_ic_t2f_mou_8<=telecom_df.std_ic_t2f_mou_8.quantile(0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.iloc[:,40:60].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 60th-80th columns\n",
    "telecom_df.iloc[:,60:80].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 60th-80th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.total_ic_mou_6<=telecom_df.total_ic_mou_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_ic_mou_7<=telecom_df.total_ic_mou_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_ic_mou_8<=telecom_df.total_ic_mou_8.quantile(0.99)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.iloc[:,60:80].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with maximum 0 values\n",
    "telecom_df=telecom_df.drop(columns=['spl_ic_mou_6','isd_ic_mou_7','isd_ic_mou_8','ic_others_6','ic_others_7','ic_others_8'])\n",
    "telecom_df.iloc[:,60:80].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with maximum 0 values\n",
    "telecom_df=telecom_df.drop(columns=['isd_ic_mou_6'])\n",
    "telecom_df.iloc[:,60:80].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 60th-80th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.total_rech_num_6<=telecom_df.total_rech_num_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_rech_num_7<=telecom_df.total_rech_num_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_rech_num_8<=telecom_df.total_rech_num_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_rech_amt_6<=telecom_df.total_rech_amt_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_rech_amt_7<=telecom_df.total_rech_amt_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.total_rech_amt_8<=telecom_df.total_rech_amt_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.max_rech_amt_6<=telecom_df.max_rech_amt_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.max_rech_amt_7<=telecom_df.max_rech_amt_7.quantile(0.99)]\n",
    "\n",
    "telecom_df.iloc[:,60:80].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers of 80th-100th columns\n",
    "telecom_df.iloc[:,80:100].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers of 80th-100th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.max_rech_amt_8<=telecom_df.max_rech_amt_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.last_day_rch_amt_8<=telecom_df.last_day_rch_amt_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.vol_2g_mb_6<=telecom_df.vol_2g_mb_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.vol_2g_mb_7<=telecom_df.vol_2g_mb_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.vol_2g_mb_8<=telecom_df.vol_2g_mb_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.vol_3g_mb_6<=telecom_df.vol_3g_mb_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.vol_3g_mb_7<=telecom_df.vol_3g_mb_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.vol_3g_mb_8<=telecom_df.vol_3g_mb_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.monthly_2g_6<=telecom_df.monthly_2g_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.monthly_2g_7<=telecom_df.monthly_2g_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.sachet_2g_6<=telecom_df.sachet_2g_6.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.sachet_2g_7<=telecom_df.sachet_2g_7.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.sachet_2g_8<=telecom_df.sachet_2g_8.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.aug_vbc_3g<=telecom_df.aug_vbc_3g.quantile(0.99)]\n",
    "\n",
    "telecom_df.iloc[:,80:100].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "telecom_df.iloc[:,80:100].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns with maximum 0 values\n",
    "telecom_df=telecom_df.drop(columns=['monthly_2g_6','monthly_2g_7','sachet_2g_6','sachet_2g_7','sachet_2g_8','total_rech_data_6','total_rech_data_7','total_rech_data_8'])\n",
    "\n",
    "\n",
    "telecom_df.iloc[:,80:].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers from 80th columns\n",
    "telecom_df=telecom_df.loc[telecom_df.jun_vbc_3g<=telecom_df.jun_vbc_3g.quantile(0.99)]\n",
    "telecom_df=telecom_df.loc[telecom_df.jul_vbc_3g<=telecom_df.jul_vbc_3g.quantile(0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers from 80th columns\n",
    "telecom_df.iloc[:,80:].describe(percentiles=[0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pecentage of churned customers\n",
    "len(telecom_df.loc[telecom_df['churn']==1])/len(telecom_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#splitting into train and test data\n",
    "df_train,df_test=train_test_split(telecom_df, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the target column\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Y_train=df_train.pop('churn')\n",
    "X_train=df_train\n",
    "print(Y_train.head())\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling train dataset\n",
    "X_train[X_train.columns]=scaler.fit_transform(X_train[X_train.columns])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting target column of test dataset\n",
    "lm = LogisticRegression(class_weight='balanced')\n",
    "Y_test=df_test.pop('churn')\n",
    "X_test=df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform scaling on test dataset\n",
    "X_test[X_test.columns]=scaler.transform(X_test[X_test.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA on the data to transform the dataset so that all features are utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=123)\n",
    "pca.fit(X_train)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plot for no of components VS total variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=[12,8])\n",
    "plt.vlines(x=35, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "plt.hlines(y=0.90, xmax=35, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.plot(var_cumu)\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above scree plot, it is observed that 90% variance is explained by 35 components.So taking n_components=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing PCA on training dataset\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "pca_final = IncrementalPCA(n_components=35)\n",
    "df_train_pca = pca_final.fit_transform(X_train)\n",
    "df_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting heatmap for the components from PCA\n",
    "corrmat = np.corrcoef(df_train_pca.transpose())\n",
    "plt.figure(figsize=[35,35])\n",
    "sns.heatmap(corrmat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing PCA on testing dataset\n",
    "df_test_pca = pca_final.transform(X_test)\n",
    "\n",
    "df_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic Regression model on the dataset after performing PCA on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_pca = LogisticRegression(class_weight='balanced')\n",
    "model_pca = learner_pca.fit(df_train_pca, Y_train)\n",
    "pred_probs_test = model_pca.predict_proba(df_test_pca)\n",
    "\"{:2.2}\".format(metrics.roc_auc_score(Y_test, pred_probs_test[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_again = PCA(0.90)\n",
    "df_train_pca2 = pca_again.fit_transform(X_train)\n",
    "df_train_pca2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_pca2 = LogisticRegression(class_weight='balanced')\n",
    "model_pca2 = learner_pca2.fit(df_train_pca2, Y_train)\n",
    "df_test_pca2 = pca_again.transform(X_test)\n",
    "df_test_pca2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "pred_probs_test2 = model_pca2.predict_proba(df_test_pca2)[:,1]\n",
    "\"{:2.2}\".format(metrics.roc_auc_score(Y_test, pred_probs_test2))\n",
    "y_train_pred_final = pd.DataFrame({'Churn':Y_test.values,'Churn_Prob':pred_probs_test2})\n",
    "y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "print(classification_report(y_train_pred_final.Churn, y_train_pred_final.predicted))\n",
    "\n",
    "print(metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted))\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted), \"\\n\")\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted)\n",
    "specificity1 = confusion[1,1]/(confusion[1,0]+confusion[1,1])\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build SVM model using linear kernel on the dataset after performing PCA on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "model_linear = SVC(class_weight='balanced',kernel='linear')\n",
    "model_linear.fit(df_train_pca2, Y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model_linear.predict(df_test_pca2)\n",
    "# confusion matrix and accuracy\n",
    "\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=Y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "# cm\n",
    "print(classification_report(Y_test,y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_true=Y_test, y_pred=y_pred))\n",
    "confusion = metrics.confusion_matrix(y_true=Y_test, y_pred=y_pred)\n",
    "specificity1 = confusion[1,1]/(confusion[1,0]+confusion[1,1])\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build SVM model using Radial Basis Function (RBF) kernel on the dataset after performing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "model_linear = SVC(class_weight='balanced',kernel='rbf')\n",
    "model_linear.fit(df_train_pca2, Y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model_linear.predict(df_test_pca2)\n",
    "# confusion matrix and accuracy\n",
    "\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=Y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "# cm\n",
    "print(classification_report(Y_test,y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_true=Y_test, y_pred=y_pred))\n",
    "confusion = metrics.confusion_matrix(y_true=Y_test, y_pred=y_pred)\n",
    "specificity1 = confusion[1,1]/(confusion[1,0]+confusion[1,1])\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters to build Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning hyperparameter max_depth\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV \n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(2, 20, 5)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   return_train_score=True,\n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(df_train_pca2, Y_train)\n",
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()\n",
    "# plotting accuracies with max_depth\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GridSearchCV to find optimal n_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'n_estimators': range(100, 3000, 400)}\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "rf = RandomForestClassifier(max_depth=4)\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                  return_train_score=True,\n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(df_train_pca2, Y_train)\n",
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()\n",
    "# plotting accuracies with n_estimators\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal max_features\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_features': [4, 8, 14, 20, 34,40]}\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "rf = RandomForestClassifier(max_depth=4)\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                  return_train_score=True,\n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(df_train_pca2, Y_train)\n",
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()\n",
    "# plotting accuracies with n_estimators\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_features\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_features\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal min_samples_leaf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_leaf': range(100, 400, 50)}\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "rf = RandomForestClassifier(max_depth=4)\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                  return_train_score=True,\n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(df_train_pca2, Y_train)\n",
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()\n",
    "# plotting accuracies with n_estimators\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal min_samples_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_split': range(200, 500, 50)}\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                  return_train_score=True,\n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(df_train_pca2, Y_train)\n",
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()\n",
    "# plotting accuracies with n_estimators\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal hyperparameters\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "param_grid = {\n",
    "    'max_depth': [4,8,10],\n",
    "    'min_samples_leaf': range(100, 400, 200),\n",
    "    'min_samples_split': range(200, 500, 200),\n",
    "    'n_estimators': [100,200, 300], \n",
    "    'max_features': [5, 10]}\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, return_train_score=True,\n",
    "                          cv = 3, n_jobs = -1,verbose = 1)\n",
    "grid_search.fit(df_train_pca2, Y_train)\n",
    "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Random Forest model with tuned hyperparameters of the dataset after performing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(class_weight='balanced',\n",
    "                             bootstrap=True,\n",
    "                             max_depth=12,\n",
    "                             min_samples_leaf=150, \n",
    "                             min_samples_split=100,\n",
    "                             max_features=20,\n",
    "                             n_estimators=500)\n",
    "rfc.fit(df_train_pca2, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = rfc.predict(df_test_pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(Y_test,predictions))\n",
    "print(confusion_matrix(Y_test,predictions))\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=Y_test, y_pred=predictions), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(Y_test,predictions)\n",
    "specificity1 = confusion[1,1]/(confusion[1,0]+confusion[1,1])\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform RFE to obtain best 15 features from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(lm, 15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train, Y_train)\n",
    "col = X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking VIFs of the top 15 variables\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column with VIF>5\n",
    "col=col.drop('total_og_mou_8')\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column with VIF>5\n",
    "col=col.drop('offnet_mou_8')\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column with VIF>5\n",
    "col=col.drop('total_amt_8')\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build GLM model to calculate the coefficients\n",
    "import statsmodels.api as sm\n",
    "X_train_new=X_train[col]\n",
    "X_train_sm = sm.add_constant(X_train_new)\n",
    "logm2 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic Regression model without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_new=X_test[col]\n",
    "X_test_sm = sm.add_constant(X_test_new)\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "model_lr = lr.fit(X_train_sm, Y_train)\n",
    "\n",
    "\n",
    "pred_probs_test2 = model_lr.predict_proba(X_test_sm)[:,1]\n",
    "\"{:2.2}\".format(metrics.roc_auc_score(Y_test, pred_probs_test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'Churn':Y_test.values,'Churn_Prob':pred_probs_test2})\n",
    "y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train_pred_final.Churn, y_train_pred_final.predicted))\n",
    "specificity1 = confusion[1,1]/(confusion[1,0]+confusion[1,1])\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here, the model to be chosen for this dataset after applying PCA is Logistic Regression since the highest sensitivity\n",
    "##### is obtained from Logistic Regression model. Since our concern is to predict the churn customers, and the factors affecting them,\n",
    "##### so the metric to be considered for this model is Sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The top 5 feature variables after performing RFE and Logistic Regression are:-\n",
    "##### 1.loc_ic_mou_8\n",
    "##### 2.last_day_rch_amt_8\n",
    "##### 3.total_rech_num_8\n",
    "##### 4.max_rech_amt_8\n",
    "##### 5.std_ic_t2f_mou_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
